

















!pip install -qU bitsandbytes datasets accelerate loralib peft transformers trl





import torch
torch.cuda.is_available()





import os
os.environ["CUDA_VISIBLE_DEVICES"]="0"
import torch
import torch.nn as nn
import bitsandbytes as bnb
from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, BitsAndBytesConfig














bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.float16,
)





model_id = "mistralai/Mistral-7B-Instruct-v0.2"

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map='auto',
)


tokenizer = AutoTokenizer.from_pretrained(model_id)


tokenizer.pad_token = tokenizer.unk_token
tokenizer.padding_side = "right"





print(model)


model.config














from datasets import load_dataset

dataset_name = "samsum"
dataset = load_dataset(dataset_name)





dataset





dataset["test"] = dataset["test"].select(range(50))


dataset["train"] = dataset["train"].select(range(1000))


dataset["validation"] = dataset["validation"].select(range(50))


dataset





dataset["train"][0]








def create_prompt(sample, include_response = True):
  """
  Parameters:
    - sample: dict representing row of dataset
    - include_response: bool

  Functionality:
    This function should build the Python str `full_prompt`.

    If `include_response` is true, it should include the summary -
    else it should not contain the summary (useful for prompting) and testing

  Returns:
    - full_prompt: str
  """

  ### YOUR CODE HERE

  return full_prompt


print(create_prompt(dataset["test"][1]))


def generate_response(prompt, model, tokenizer):
  """
  Parameters:
    - prompt: str representing formatted prompt
    - model: model object
    - tokenizer: tokenizer object

  Functionality:
    This will allow our model to generate a response to a prompt!

  Returns:
    - str response of the model
  """

  # convert str input into tokenized input
  encoded_input = tokenizer(prompt,  return_tensors="pt")

  # send the tokenized inputs to our GPU
  model_inputs = encoded_input.to('cuda')

  # generate response and set desired generation parameters
  generated_ids = model.generate(
      **model_inputs,
      max_new_tokens=256,
      do_sample=True,
      pad_token_id=tokenizer.eos_token_id
  )

  # decode output from tokenized output to str output
  decoded_output = tokenizer.batch_decode(generated_ids)

  # return only the generated response (not the prompt) as output
  return decoded_output[0].split("[/INST]")[-1]


generate_response(create_prompt(dataset["test"][1], include_response=False),
                  model,
                  tokenizer)


# Ground Truth Summary
dataset["test"][1]["summary"]





generate_response(create_prompt(dataset["test"][3], include_response=False),
                  model,
                  tokenizer)


# Ground Truth Summary
dataset["test"][3]["summary"]








from peft import prepare_model_for_kbit_training
model.config.use_cache = False
model = prepare_model_for_kbit_training(model)








def print_trainable_parameters(model):
    """
    Prints the number of trainable parameters in the model.
    """
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}"
    )





from peft import LoraConfig, get_peft_model

# set our rank (higher value is more memory/better performance)
lora_r = 16

# set our dropout (default value)
lora_dropout = 0.1

# rule of thumb: alpha should be (lora_r * 2)
lora_alpha = 32

# construct our LoraConfig with the above hyperparameters
peft_config = LoraConfig(
    lora_alpha=lora_alpha,
    lora_dropout=lora_dropout,
    r=lora_r,
    bias="none",
    task_type="CAUSAL_LM"
)





model = get_peft_model(
    model,
    peft_config
)

print_trainable_parameters(model)


print(model)











from transformers import TrainingArguments

args = TrainingArguments(
  output_dir = "mistral7binstruct_summarize",
  #num_train_epochs=5,
  max_steps = 50, # comment out this line if you want to train in epochs
  per_device_train_batch_size = 1,
  warmup_steps = 0.03,
  logging_steps=10,
  #evaluation_strategy="epoch",
  evaluation_strategy="steps",
  eval_steps=25, # comment out this line if you want to evaluate at the end of each epoch
  learning_rate=2e-4,
  lr_scheduler_type='constant',
)





from trl import SFTTrainer

max_seq_length = 2048

trainer = SFTTrainer(
  model=model,
  peft_config=peft_config,
  max_seq_length=max_seq_length,
  tokenizer=tokenizer,
  packing=True,
  formatting_func=create_prompt,
  args=args,
  train_dataset=dataset["train"],
  eval_dataset=dataset["validation"]
)





trainer.train()








from huggingface_hub import notebook_login

notebook_login()


trainer.push_to_hub("ai-maker-space/mistral-7binstruct-summary-100s")





merged_model = model.merge_and_unload()





generate_response(create_prompt(dataset["test"][1], include_response=False),
                  merged_model,
                  tokenizer)








print(dataset["test"][3]["dialogue"])


generate_response(create_prompt(dataset["test"][3], include_response=False),
                  merged_model,
                  tokenizer)


# Ground Truth Summary
dataset["test"][3]["summary"]



